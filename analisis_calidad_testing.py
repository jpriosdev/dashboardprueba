"""
AnÃ¡lisis de MÃ©tricas de Calidad y Testing - JIRA LTI
Experto en AnalÃ­tica de Datos para Director de TecnologÃ­a
"""

import pandas as pd
import numpy as np
from collections import Counter
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class JiraQualityAnalyzer:
    """
    Analizador especializado en calidad y testing para datasets de JIRA
    """
    
    def __init__(self, csv_path):
        """Inicializar con ruta al archivo CSV"""
        self.csv_path = csv_path
        self.df = None
        self.load_data()
    
    def load_data(self):
        """Cargar y preparar datos"""
        print("ğŸ“‚ Cargando datos JIRA LTI...")
        self.df = pd.read_csv(self.csv_path)
        print(f"âœ“ {len(self.df)} registros cargados")
        print(f"âœ“ {len(self.df.columns)} columnas")
    
    def get_overview(self):
        """Resumen general del proyecto"""
        print("\n" + "="*70)
        print("ğŸ“Š RESUMEN GENERAL DEL PROYECTO")
        print("="*70)
        
        # Issues por tipo
        print("\n1. DISTRIBUCIÃ“N DE TIPOS DE ISSUES:")
        issue_types = self.df['Issue Type'].value_counts()
        for issue_type, count in issue_types.items():
            pct = (count / len(self.df)) * 100
            print(f"   â€¢ {issue_type}: {count} ({pct:.1f}%)")
        
        # Estados
        print("\n2. DISTRIBUCIÃ“N DE ESTADOS:")
        statuses = self.df['Status'].value_counts()
        for status, count in statuses.items():
            pct = (count / len(self.df)) * 100
            print(f"   â€¢ {status}: {count} ({pct:.1f}%)")
        
        # Prioridades
        print("\n3. DISTRIBUCIÃ“N DE PRIORIDADES:")
        priorities = self.df['Priority'].value_counts()
        for priority, count in priorities.items():
            if pd.notna(priority):
                pct = (count / len(self.df)) * 100
                print(f"   â€¢ {priority}: {count} ({pct:.1f}%)")
    
    def calculate_test_coverage_rate(self):
        """
        MÃ‰TRICA 1: Test Coverage Rate
        Porcentaje de historias con test execution asociado
        """
        print("\n" + "="*70)
        print("ğŸ§ª MÃ‰TRICA 1: TEST COVERAGE RATE (TCR)")
        print("="*70)
        
        stories = self.df[self.df['Issue Type'] == 'Story']
        test_execs = self.df[self.df['Issue Type'] == 'Test Execution']
        
        # Issues que tiene relaciÃ³n Test
        issues_with_tests = self.df[
            self.df['Inward issue link (Test)'].notna() | 
            self.df['Outward issue link (Test)'].notna()
        ]
        
        tcr = (len(issues_with_tests) / len(self.df)) * 100 if len(self.df) > 0 else 0
        
        print(f"\nâœ“ Total Issues con Test asociado: {len(issues_with_tests)}")
        print(f"âœ“ Total Stories: {len(stories)}")
        print(f"âœ“ Total Test Executions: {len(test_execs)}")
        print(f"\nğŸ“ˆ TEST COVERAGE RATE: {tcr:.1f}%")
        print(f"ğŸ¯ Meta recomendada: >80%")
        print(f"âš ï¸  Estado: {'CRÃTICO' if tcr < 50 else 'MEJORABLE' if tcr < 80 else 'ACEPTABLE'}")
        
        return tcr
    
    def calculate_defect_escape_rate(self):
        """
        MÃ‰TRICA 2: Defect Escape Rate
        Bugs encontrados en producciÃ³n vs total de bugs
        """
        print("\n" + "="*70)
        print("ğŸ› MÃ‰TRICA 2: DEFECT ESCAPE RATE (DER)")
        print("="*70)
        
        bugs = self.df[self.df['Issue Type'] == 'Bug']
        bugs_done = bugs[bugs['Status'] == 'Done']
        bugs_in_prod = bugs_done[bugs_done['Status'] == 'Done'].copy()
        
        # Checks si tiene "Deployed to Production" en el log de trabajo
        deployed_bugs = 0
        for idx, row in bugs_in_prod.iterrows():
            if pd.notna(row.get('Log Work', '')):
                if 'Deployed to Production' in str(row.get('Log Work', '')):
                    deployed_bugs += 1
        
        der = (deployed_bugs / len(bugs)) * 100 if len(bugs) > 0 else 0
        
        print(f"\nâœ“ Total Bugs registrados: {len(bugs)}")
        print(f"âœ“ Bugs completados (Done): {len(bugs_done)}")
        print(f"âœ“ Bugs escapados a producciÃ³n: {deployed_bugs}")
        print(f"\nğŸ“ˆ DEFECT ESCAPE RATE: {der:.1f}%")
        print(f"ğŸ¯ Meta recomendada: <5%")
        print(f"âš ï¸  Estado: {'CRÃTICO' if der > 10 else 'MEJORABLE' if der > 5 else 'ACEPTABLE'}")
        
        return der, len(bugs)
    
    def calculate_test_execution_velocity(self):
        """
        MÃ‰TRICA 3: Test Execution Velocity
        Test cases ejecutados por dÃ­a de sprint
        """
        print("\n" + "="*70)
        print("âš¡ MÃ‰TRICA 3: TEST EXECUTION VELOCITY (TEV)")
        print("="*70)
        
        test_execs = self.df[self.df['Issue Type'] == 'Test Execution']
        test_execs_done = test_execs[test_execs['Status'].isin(['Done', 'In Testing', 'Ready for Testing'])]
        
        # AnÃ¡lisis por sprint
        sprints = self.df['Sprint'].dropna().unique()
        print(f"\nâœ“ Sprints activos: {len(sprints)}")
        
        # Sprints identificados
        sprint_data = []
        for sprint in sprints[:5]:  # Top 5 sprints
            if pd.notna(sprint):
                sprint_tests = test_execs_done[test_execs_done['Sprint'] == sprint]
                sprint_data.append({
                    'sprint': str(sprint)[:30],
                    'tests': len(sprint_tests)
                })
        
        avg_tests_per_sprint = len(test_execs_done) / len(sprints) if len(sprints) > 0 else 0
        tev = avg_tests_per_sprint / 14  # Asumiendo sprints de 2 semanas
        
        print(f"\nâœ“ Test Executions completadas: {len(test_execs_done)}")
        print(f"âœ“ Promedio por sprint: {avg_tests_per_sprint:.1f}")
        print(f"\nğŸ“ˆ TEST EXECUTION VELOCITY: {tev:.2f} tests/dÃ­a")
        print(f"ğŸ¯ Meta recomendada: >1.0 tests/dÃ­a")
        print(f"âš ï¸  Estado: {'CRÃTICO' if tev < 0.5 else 'MEJORABLE' if tev < 1.0 else 'ACEPTABLE'}")
        
        return tev, len(test_execs_done)
    
    def calculate_quality_gate_pass_rate(self):
        """
        MÃ‰TRICA 4: Quality Gate Pass Rate
        Issues que cumplen criterios sin rechazo
        """
        print("\n" + "="*70)
        print("âœ… MÃ‰TRICA 4: QUALITY GATE PASS RATE (QGPR)")
        print("="*70)
        
        issues_done = self.df[self.df['Status'] == 'Done']
        
        # Issues que fueron rechazadas (tienen estado "Reviewed" antes de Done)
        # AproximaciÃ³n: si tienen mÃºltiples transiciones de estado
        rejected_pattern = self.df[
            self.df['Status Category'].isin(['In Progress', 'To Do', 'Reviewed'])
        ]
        
        # Issues que pasaron directamente
        clean_issues = issues_done[~issues_done['Issue id'].isin(rejected_pattern['Issue id'])]
        
        qgpr = (len(clean_issues) / len(self.df)) * 100 if len(self.df) > 0 else 0
        
        print(f"\nâœ“ Total issues completadas (Done): {len(issues_done)}")
        print(f"âœ“ Issues sin rechazo: {len(clean_issues)}")
        print(f"âœ“ Issues con rework: {len(rejected_pattern)}")
        print(f"\nğŸ“ˆ QUALITY GATE PASS RATE: {qgpr:.1f}%")
        print(f"ğŸ¯ Meta recomendada: >70%")
        print(f"âš ï¸  Estado: {'CRÃTICO' if qgpr < 50 else 'MEJORABLE' if qgpr < 70 else 'ACEPTABLE'}")
        
        return qgpr
    
    def analyze_bug_severity(self):
        """
        MÃ‰TRICA 5: Bug Severity Distribution
        AnÃ¡lisis de severidad de bugs
        """
        print("\n" + "="*70)
        print("ğŸš¨ MÃ‰TRICA 5: BUG SEVERITY DISTRIBUTION (BSD)")
        print("="*70)
        
        bugs = self.df[self.df['Issue Type'] == 'Bug']
        
        # Clasificar por severidad basado en:
        # 1. Priority
        # 2. Bloqueadores (Inward Blocks)
        # 3. DescripciÃ³n
        
        critical_keywords = ['crash', 'security', 'data loss', 'blocks', 'critical']
        high_keywords = ['error', 'broken', 'defect', 'failure']
        
        critical_count = 0
        high_count = 0
        medium_count = 0
        low_count = 0
        
        for idx, row in bugs.iterrows():
            priority = str(row.get('Priority', '')).lower()
            description = str(row.get('Description', '')).lower()
            summary = str(row.get('Summary', '')).lower()
            
            text = description + ' ' + summary
            
            if 'critical' in priority or any(kw in text for kw in critical_keywords):
                critical_count += 1
            elif 'high' in priority or any(kw in text for kw in high_keywords):
                high_count += 1
            elif 'medium' in priority:
                medium_count += 1
            else:
                low_count += 1
        
        total_bugs = len(bugs)
        
        print(f"\nâœ“ Total Bugs: {total_bugs}")
        print(f"\nğŸ“Š DistribuciÃ³n de severidad:")
        
        if total_bugs > 0:
            critical_pct = (critical_count / total_bugs) * 100
            high_pct = (high_count / total_bugs) * 100
            medium_pct = (medium_count / total_bugs) * 100
            low_pct = (low_count / total_bugs) * 100
            
            print(f"   ğŸ”´ Critical: {critical_count} ({critical_pct:.1f}%)")
            print(f"   ğŸŸ  High: {high_count} ({high_pct:.1f}%)")
            print(f"   ğŸŸ¡ Medium: {medium_count} ({medium_pct:.1f}%)")
            print(f"   ğŸŸ¢ Low: {low_count} ({low_pct:.1f}%)")
            
            print(f"\nğŸ¯ Metas recomendadas:")
            print(f"   â€¢ Critical: 0% (Actual: {critical_pct:.1f}%)")
            print(f"   â€¢ High: <5% (Actual: {high_pct:.1f}%)")
            print(f"   â€¢ Medium: <15% (Actual: {medium_pct:.1f}%)")
            print(f"   â€¢ Low: >80% (Actual: {low_pct:.1f}%)")
            
            health = 'CRÃTICO' if critical_pct > 5 else 'MEJORABLE' if high_pct > 10 else 'ACEPTABLE'
            print(f"\nâš ï¸  Estado: {health}")
        
        return {
            'critical': critical_count,
            'high': high_count,
            'medium': medium_count,
            'low': low_count
        }
    
    def analyze_time_metrics(self):
        """AnÃ¡lisis de tiempos de resoluciÃ³n"""
        print("\n" + "="*70)
        print("â±ï¸  ANÃLISIS DE TIEMPOS DE RESOLUCIÃ“N")
        print("="*70)
        
        # AnÃ¡lisis de columna "Time Spent"
        time_spent = pd.to_numeric(self.df['Time Spent'], errors='coerce')
        
        print(f"\nâœ“ Total tiempo registrado: {time_spent.sum():.0f} horas")
        print(f"âœ“ Promedio por issue: {time_spent.mean():.1f} horas")
        print(f"âœ“ Mediana: {time_spent.median():.1f} horas")
        print(f"âœ“ MÃ¡ximo: {time_spent.max():.0f} horas")
        print(f"âœ“ MÃ­nimo: {time_spent.min():.0f} horas")
        
        # Por tipo de issue
        print(f"\nğŸ“Š Tiempo promedio por tipo de issue:")
        for issue_type in self.df['Issue Type'].unique():
            if pd.notna(issue_type):
                type_time = time_spent[self.df['Issue Type'] == issue_type].mean()
                if not pd.isna(type_time):
                    print(f"   â€¢ {issue_type}: {type_time:.1f} horas")
    
    def generate_report(self):
        """Generar reporte completo"""
        print("\n\n")
        print("â•”" + "="*68 + "â•—")
        print("â•‘" + " "*15 + "REPORTE DE CALIDAD Y TESTING JIRA LTI" + " "*17 + "â•‘")
        print("â•š" + "="*68 + "â•")
        
        self.get_overview()
        tcr = self.calculate_test_coverage_rate()
        der, total_bugs = self.calculate_defect_escape_rate()
        tev, test_execs = self.calculate_test_execution_velocity()
        qgpr = self.calculate_quality_gate_pass_rate()
        bsd = self.analyze_bug_severity()
        self.analyze_time_metrics()
        
        # Resumen ejecutivo
        print("\n\n" + "="*70)
        print("ğŸ“‹ RESUMEN EJECUTIVO PARA DIRECTOR")
        print("="*70)
        
        print(f"""
â•”â”€ KPIs CRÃTICOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•—
â”‚                                                                  â”‚
â”‚  1. Test Coverage Rate:        {tcr:>6.1f}%  [Meta: 80%]
â”‚  2. Defect Escape Rate:        {der:>6.1f}%  [Meta: <5%]
â”‚  3. Test Execution Velocity:   {tev:>6.2f}   tests/dÃ­a [Meta: >1.0]
â”‚  4. Quality Gate Pass Rate:    {qgpr:>6.1f}%  [Meta: 70%]
â”‚                                                                  â”‚
â”‚  Total Bugs: {total_bugs:<3} | Test Executions: {test_execs:<3}              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
        print("\nğŸ¯ RECOMENDACIONES INMEDIATAS:")
        print("""
1. IMPLEMENTAR TEST-FIRST APPROACH
   â†’ Crear Test Cases ANTES de "In Development"
   â†’ Responsable: QA Lead
   â†’ Timeline: Inmediato
   
2. OPTIMIZAR VELOCIDAD DE TESTING  
   â†’ Target: 0.21 â†’ 1.0 tests/dÃ­a (+380% mejora)
   â†’ Estrategia: AutomatizaciÃ³n + paralelizaciÃ³n
   â†’ Timeline: 30 dÃ­as
   
3. ESTABLECER DEFINITION OF DONE
   â†’ Checklist: Test execution + Quality gate + 0 Critical bugs
   â†’ Enforcement: No merge sin cumplir
   â†’ Timeline: Inmediato
   
4. MONITOREAR KPIs SEMANALMENTE
   â†’ Dashboard actualizado cada lunes
   â†’ Alertas: DER >5%, TCR <70%
   â†’ Escalation: Director si hay 2 semanas bajo meta
        """)
        
        print("\n" + "="*70)


def main():
    """Punto de entrada"""
    try:
        analyzer = JiraQualityAnalyzer(
            r"c:\Users\ultra\PycharmProjects\PythonProject\TablerosLTI\JIRA LTI.csv"
        )
        analyzer.generate_report()
        
    except FileNotFoundError:
        print("âŒ Error: No se encontrÃ³ el archivo JIRA LTI.csv")
    except Exception as e:
        print(f"âŒ Error durante anÃ¡lisis: {str(e)}")


if __name__ == "__main__":
    main()
