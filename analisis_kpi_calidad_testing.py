"""
ANÃLISIS EJECUTIVO DE MÃ‰TRICAS KPI PARA CALIDAD Y TESTING
AnÃ¡lisis del archivo JIRA LTI para Director de TecnologÃ­a
"""

import pandas as pd
import numpy as np
from datetime import datetime
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Cargar datos
df = pd.read_csv('JIRA LTI.csv', low_memory=False)

print("=" * 100)
print("ANÃLISIS EJECUTIVO: MÃ‰TRICAS KPI DE CALIDAD Y TESTING - JIRA LTI")
print("=" * 100)
print(f"\nFecha del anÃ¡lisis: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
print(f"Total de tickets analizados: {len(df):,}")
print(f"PerÃ­odo cubierto: {df['Created'].min()} a {df['Created'].max()}")

# ============================================================================
# 1. ANÃLISIS GENERAL DE TIPOS DE ISSUES
# ============================================================================
print("\n" + "=" * 100)
print("1. DISTRIBUCIÃ“N DE TIPOS DE ISSUES Y COBERTURA DE TESTING")
print("=" * 100)

issue_type_counts = df['Issue Type'].value_counts()
print("\nVolumen de Issues por Tipo:")
for issue_type, count in issue_type_counts.items():
    pct = (count / len(df)) * 100
    print(f"  â€¢ {issue_type}: {count:,} ({pct:.1f}%)")

# Categorizar issues
testing_issues = df[df['Issue Type'].isin(['Test', 'Test Execution', 'Test Plan'])].shape[0]
development_issues = df[df['Issue Type'].isin(['Story', 'Task', 'Sub-task'])].shape[0]
bug_issues = df[df['Issue Type'] == 'Bug'].shape[0]

print(f"\nRESUMEN CRÃTICO:")
print(f"  â€¢ Issues de TESTING: {testing_issues:,} ({(testing_issues/len(df)*100):.1f}%)")
print(f"  â€¢ Issues de DESARROLLO: {development_issues:,} ({(development_issues/len(df)*100):.1f}%)")
print(f"  â€¢ Issues de BUGS: {bug_issues:,} ({(bug_issues/len(df)*100):.1f}%)")

testing_ratio = testing_issues / len(df)
print(f"\nâš ï¸  RATIO TESTING:TOTAL = {testing_ratio:.2%}")
print(f"   InterpretaciÃ³n: Por cada 100 tickets, {testing_ratio*100:.1f} son de testing")

# ============================================================================
# 2. ANÃLISIS DE ESTADOS Y VELOCIDAD DE ENTREGA
# ============================================================================
print("\n" + "=" * 100)
print("2. ESTADO DE COMPLETITUD Y VELOCIDAD DE ENTREGA")
print("=" * 100)

status_counts = df['Status'].value_counts()
print("\nDistribuciÃ³n por Estado:")
for status, count in status_counts.items():
    pct = (count / len(df)) * 100
    print(f"  â€¢ {status}: {count:,} ({pct:.1f}%)")

# Calcular tasa de completitud
done_issues = df[df['Status'] == 'Done'].shape[0]
completion_rate = (done_issues / len(df)) * 100
print(f"\nâœ“ TASA DE COMPLETITUD: {completion_rate:.1f}%")
print(f"   ({done_issues:,} de {len(df):,} tickets completados)")

# Issues pendientes
pending_statuses = ['To Do', 'In Development', 'In Progress', 'Ready for Testing', 'In Testing']
pending = df[df['Status'].isin(pending_statuses)].shape[0]
print(f"\nâ³ ISSUES PENDIENTES: {pending:,} ({(pending/len(df)*100):.1f}%)")

# ============================================================================
# 3. ANÃLISIS DE BUGS Y DEFECTOS
# ============================================================================
print("\n" + "=" * 100)
print("3. ANÃLISIS DE BUGS, DEFECTOS Y SEVERIDAD")
print("=" * 100)

bugs_total = bug_issues
print(f"\nTotal de BUGS: {bugs_total:,}")

# ProporciÃ³n bugs respecto a development
if development_issues > 0:
    bug_ratio = bugs_total / development_issues
    print(f"RATIO BUG:DEVELOPMENT = {bug_ratio:.2f}")
    print(f"   InterpretaciÃ³n: Por cada feature de desarrollo, hay {bug_ratio:.2f} bugs")

# Estado de bugs
bug_df = df[df['Issue Type'] == 'Bug']
bug_status = bug_df['Status'].value_counts()
print(f"\nEstado de BUGS:")
for status, count in bug_status.items():
    pct = (count / bugs_total) * 100
    print(f"  â€¢ {status}: {count} ({pct:.1f}%)")

bugs_resolved = bug_df[bug_df['Status'] == 'Done'].shape[0]
bug_resolution_rate = (bugs_resolved / bugs_total * 100) if bugs_total > 0 else 0
print(f"\nâœ“ TASA DE RESOLUCIÃ“N DE BUGS: {bug_resolution_rate:.1f}%")
print(f"   ({bugs_resolved:,} de {bugs_total:,} bugs resueltos)")

# AnÃ¡lisis de severidad
print(f"\nPrioridad de BUGS:")
bug_priority = bug_df['Priority'].value_counts()
for priority, count in bug_priority.items():
    pct = (count / bugs_total) * 100
    print(f"  â€¢ {priority}: {count} ({pct:.1f}%)")

# ============================================================================
# 4. ANÃLISIS DE TEST COVERAGE Y TEST EXECUTION
# ============================================================================
print("\n" + "=" * 100)
print("4. TEST COVERAGE Y TEST EXECUTION")
print("=" * 100)

tests = df[df['Issue Type'] == 'Test']
test_executions = df[df['Issue Type'] == 'Test Execution']

print(f"\nTest Cases creados: {len(tests):,}")
print(f"Test Executions: {len(test_executions):,}")

if len(tests) > 0:
    test_exec_ratio = len(test_executions) / len(tests) if len(tests) > 0 else 0
    print(f"\nğŸ“Š EXECUTION COVERAGE: {test_exec_ratio:.2f}x")
    print(f"   (Ratio de execuciones respecto a test cases)")

# Estado de test executions
if len(test_executions) > 0:
    test_exec_status = test_executions['Status'].value_counts()
    print(f"\nEstado de TEST EXECUTIONS:")
    for status, count in test_exec_status.items():
        pct = (count / len(test_executions)) * 100
        print(f"  â€¢ {status}: {count} ({pct:.1f}%)")
    
    test_exec_done = test_executions[test_executions['Status'] == 'Done'].shape[0]
    test_exec_completion = (test_exec_done / len(test_executions)) * 100
    print(f"\nâœ“ TASA DE EJECUCIÃ“N COMPLETADA: {test_exec_completion:.1f}%")

# ============================================================================
# 5. ANÃLISIS DE RELACIONES Y DEPENDENCIAS
# ============================================================================
print("\n" + "=" * 100)
print("5. BLOQUEOS Y DEPENDENCIAS (RIESGO)")
print("=" * 100)

# Contar issue links (Blocks, Defect, Test, Clone)
blocks_inward = df['Inward issue link (Blocks)'].notna().sum()
blocks_outward = df['Outward issue link (Blocks)'].notna().sum()
total_blocks = blocks_inward + blocks_outward

defect_links = df['Inward issue link (Defect)'].notna().sum() + df['Outward issue link (Defect)'].notna().sum()
test_links = df[df.columns[df.columns.str.contains('Inward issue link \\(Test')]].notna().sum().sum()
test_links += df[df.columns[df.columns.str.contains('Outward issue link \\(Test')]].notna().sum().sum()

print(f"\nğŸ”— RELACIONES DE DEPENDENCIAS:")
print(f"  â€¢ Issues BLOQUEADOS (Blocks): {total_blocks}")
print(f"  â€¢ Links de DEFECTOS: {defect_links}")
print(f"  â€¢ Links de TESTS: {test_links}")

if total_blocks > 0:
    blocked_pct = (total_blocks / len(df)) * 100
    print(f"\nâš ï¸  {blocked_pct:.1f}% de tickets tienen bloqueos activos")
    print(f"   RIESGO: Esto impacta la velocidad de entrega")

# ============================================================================
# 6. ANÃLISIS TEMPORAL (Velocidad de ResoluciÃ³n)
# ============================================================================
print("\n" + "=" * 100)
print("6. VELOCIDAD DE RESOLUCIÃ“N (Eficiencia Operacional)")
print("=" * 100)

# Convertir fechas
df['Created'] = pd.to_datetime(df['Created'], errors='coerce')
df['Resolved'] = pd.to_datetime(df['Resolved'], errors='coerce')

# Calcular tiempo de resoluciÃ³n
resolved_df = df[df['Resolved'].notna()].copy()
if len(resolved_df) > 0:
    resolved_df['Time_to_Resolve_Days'] = (resolved_df['Resolved'] - resolved_df['Created']).dt.days
    avg_resolution_time = resolved_df['Time_to_Resolve_Days'].mean()
    median_resolution_time = resolved_df['Time_to_Resolve_Days'].median()
    
    print(f"\nâ±ï¸  TIEMPO PROMEDIO DE RESOLUCIÃ“N:")
    print(f"  â€¢ Promedio: {avg_resolution_time:.1f} dÃ­as")
    print(f"  â€¢ Mediana: {median_resolution_time:.1f} dÃ­as")
    print(f"  â€¢ Issues resueltos: {len(resolved_df):,} ({(len(resolved_df)/len(df)*100):.1f}%)")
    
    # Por tipo de issue
    print(f"\nTiempo de resoluciÃ³n por TIPO DE ISSUE:")
    for issue_type in ['Bug', 'Story', 'Task', 'Test Execution']:
        type_resolved = resolved_df[resolved_df['Issue Type'] == issue_type]
        if len(type_resolved) > 0:
            avg_time = type_resolved['Time_to_Resolve_Days'].mean()
            print(f"  â€¢ {issue_type}: {avg_time:.1f} dÃ­as ({len(type_resolved)} tickets)")

# ============================================================================
# 7. ANÃLISIS DE ESTIMACIONES Y TRABAJO REAL
# ============================================================================
print("\n" + "=" * 100)
print("7. ESTIMACIONES VS. TRABAJO REAL (PrecisiÃ³n en PlanificaciÃ³n)")
print("=" * 100)

# Columnas relevantes
estimate_col = 'Original estimate'
spent_col = 'Time Spent'

if estimate_col in df.columns and spent_col in df.columns:
    # Limpiar datos
    df['Original estimate'] = pd.to_numeric(df['Original estimate'], errors='coerce')
    df['Time Spent'] = pd.to_numeric(df['Time Spent'], errors='coerce')
    
    estimated = df[df['Original estimate'].notna()].shape[0]
    with_time_spent = df[df['Time Spent'].notna()].shape[0]
    
    print(f"\nTARGETING Y ESTIMACIONES:")
    print(f"  â€¢ Tickets con estimaciÃ³n: {estimated:,} ({(estimated/len(df)*100):.1f}%)")
    print(f"  â€¢ Tickets con tiempo registrado: {with_time_spent:,} ({(with_time_spent/len(df)*100):.1f}%)")
    
    # Calcular accuracy
    comparison_df = df[(df['Original estimate'].notna()) & (df['Time Spent'].notna())].copy()
    if len(comparison_df) > 0:
        comparison_df['Accuracy_Ratio'] = comparison_df['Time Spent'] / comparison_df['Original estimate']
        avg_accuracy = comparison_df['Accuracy_Ratio'].mean()
        print(f"\nğŸ“Š PRECISIÃ“N EN ESTIMACIONES:")
        print(f"  â€¢ Ratio Trabajo_Real:Estimado = {avg_accuracy:.2f}x")
        if avg_accuracy > 1.2:
            print(f"    âš ï¸  RIESGO: Tareas se extienden {(avg_accuracy-1)*100:.0f}% mÃ¡s que lo estimado")
        elif avg_accuracy < 0.8:
            print(f"    âœ“ Bueno: Tareas se completan {(1-avg_accuracy)*100:.0f}% mÃ¡s rÃ¡pido que lo estimado")

# ============================================================================
# 8. ANÃLISIS DE EQUIPOS Y ASIGNACIÃ“N
# ============================================================================
print("\n" + "=" * 100)
print("8. DISTRIBUCIÃ“N DE CARGA POR EQUIPO/PERSONA")
print("=" * 100)

assignees = df['Assignee'].value_counts().head(10)
print(f"\nTop 10 personas asignadas a tickets:")
for assignee, count in assignees.items():
    if pd.notna(assignee):
        pct = (count / len(df)) * 100
        print(f"  â€¢ {assignee}: {count:,} ({pct:.1f}%)")

# ============================================================================
# 9. ANÃLISIS DE RESOLUCIONES
# ============================================================================
print("\n" + "=" * 100)
print("9. ANÃLISIS DE RESOLUCIONES")
print("=" * 100)

resolutions = df['Resolution'].value_counts()
print(f"\nResoluciones registradas:")
for resolution, count in resolutions.head(10).items():
    if pd.notna(resolution):
        pct = (count / len(df)) * 100
        print(f"  â€¢ {resolution}: {count:,} ({pct:.1f}%)")

# ============================================================================
# MÃ‰TRICAS KPI RECOMENDADAS
# ============================================================================
print("\n" + "=" * 100)
print("MÃ‰TRICAS KPI RECOMENDADAS PARA DIRECTOR DE TECNOLOGÃA")
print("=" * 100)

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        KPI #1: COBERTURA DE TESTING                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š DEFINICIÃ“N:
   Porcentaje de features de desarrollo que tienen casos de prueba asociados

ğŸ”¢ FÃ“RMULA:
   Test Coverage = (Test Cases Asociados / Total de Features de Desarrollo) Ã— 100%

ğŸ“ˆ VALOR ACTUAL:
   Test Cases: {test_count}
   Features: {feature_count}
   COBERTURA: {coverage:.1f}%

ğŸ¯ INSIGHT ESTRATÃ‰GICO:
   â€¢ Indica quÃ© porcentaje del cÃ³digo en desarrollo tiene cobertura de testing
   â€¢ Valores < 70%: Riesgo de defectos en producciÃ³n
   â€¢ Valores > 90%: Excelente control de calidad

ğŸ“‹ RECOMENDACIONES:
   âœ“ Establecer target mÃ­nimo de 85% de cobertura
   âœ“ Revisar features sin tests asociados
   âœ“ Implementar polÃ­tica "Definition of Done" = tests + code

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   KPI #2: TASA DE DEFECTOS (BUG DENSITY)                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š DEFINICIÃ“N:
   NÃºmero de bugs reportados por cada unidad de trabajo completada

ğŸ”¢ FÃ“RMULA:
   Bug Density = (Total Bugs / (Total Completed Stories + Total Completed Tasks)) Ã— 100

ğŸ“ˆ VALOR ACTUAL:
   Total Bugs: {total_bugs}
   Features Completadas: {completed_features}
   BUG DENSITY: {bug_density:.2f} bugs/feature

ğŸ¯ INSIGHT ESTRATÃ‰GICO:
   â€¢ Refleja la calidad intrÃ­nseca del desarrollo
   â€¢ Bugs altos = necesidad de mejorar procesos de QA
   â€¢ Tendencia: Monitor mensual para identificar degradaciÃ³n

ğŸ“‹ RECOMENDACIONES:
   âœ“ Establecer baseline y target mÃ¡ximo (ej: 0.15 bugs/feature)
   âœ“ Implementar peer code reviews antes de merge
   âœ“ Aumentar test automatizados si densidad > 0.25

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  KPI #3: EFICIENCIA DE RESOLUCIÃ“N DE DEFECTOS                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š DEFINICIÃ“N:
   Tiempo promedio para resolver bugs desde su reporte hasta resoluciÃ³n

ğŸ”¢ FÃ“RMULA:
   Mean Time to Resolution (MTTR) = Suma(Fecha_ResoluciÃ³n - Fecha_Reporte) / Total_Bugs_Resueltos

ğŸ“ˆ VALOR ACTUAL:
   MTTR Promedio: {mttr_avg:.1f} dÃ­as
   MTTR Mediana: {mttr_median:.1f} dÃ­as
   Bugs Resueltos: {bugs_resolved}/{total_bugs} ({resolution_pct:.1f}%)

ğŸ¯ INSIGHT ESTRATÃ‰GICO:
   â€¢ Mide la agilidad del equipo QA en responder a defectos
   â€¢ MTTR < 3 dÃ­as: Excelente
   â€¢ MTTR > 7 dÃ­as: Requiere mejora (riesgo de acumulaciÃ³n)
   â€¢ Correlaciona con tasa de defectos: MTTR alto + bugs altos = problemas crÃ­ticos

ğŸ“‹ RECOMENDACIONES:
   âœ“ Establecer SLA mÃ¡ximo de 5 dÃ­as para bugs crÃ­ticos
   âœ“ Priorizar bugs por severidad en sprint
   âœ“ Investigar bugs con MTTR > 10 dÃ­as

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              KPI #4: COMPLETITUD DE TEST EXECUTIONS (TESTING VELOCITY)            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š DEFINICIÃ“N:
   Porcentaje de test executions completadas vs. totales creadas

ğŸ”¢ FÃ“RMULA:
   Testing Completion Rate = (Test Executions Done / Total Test Executions) Ã— 100%

ğŸ“ˆ VALOR ACTUAL:
   Test Executions Completadas: {test_exec_completed}
   Total Test Executions: {test_exec_total}
   TASA: {test_completion_rate:.1f}%

ğŸ¯ INSIGHT ESTRATÃ‰GICO:
   â€¢ Indica si el ciclo de testing se completa dentro del sprint
   â€¢ Tasa < 80%: Testing puede ser cuello de botella
   â€¢ Tasa > 95%: Excelente ejecuciÃ³n y completitud

ğŸ“‹ RECOMENDACIONES:
   âœ“ Target: MÃ­nimo 90% de completitud antes de cierre de sprint
   âœ“ Investigar causas de incompletitud (no son defectos, sino ejecuciÃ³n no finalizada)
   âœ“ Aumentar recursos QA si tasa consistentemente < 85%

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    KPI #5: ÃNDICE DE BLOQUEOS (RISK INDEX)                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š DEFINICIÃ“N:
   Porcentaje de tickets que tienen dependencias que los bloquean

ğŸ”¢ FÃ“RMULA:
   Blocking Index = (Tickets con Issue Links de Bloqueo / Total Tickets) Ã— 100%

ğŸ“ˆ VALOR ACTUAL:
   Tickets Bloqueados: {blocked_count}
   Total Tickets: {total_tickets}
   ÃNDICE: {blocking_pct:.1f}%

ğŸ¯ INSIGHT ESTRATÃ‰GICO:
   â€¢ Mide el nivel de interdependencias en el trabajo
   â€¢ Alto bloqueo (> 15%) = PlanificaciÃ³n deficiente o arquitectura acoplada
   â€¢ Riesgo: Impacta directamente en velocidad de entrega
   â€¢ Debe monitorearse semanalmente para identificar cuellos de botella

ğŸ“‹ RECOMENDACIONES:
   âœ“ Target: Mantener < 10% de tickets bloqueados
   âœ“ Implementar daily standup enfocado en desbloqueos
   âœ“ Refactorizar dependencias para reducir acoplamiento
   âœ“ Asignar ownership claro para cada bloqueo

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""".format(
    test_count=len(tests),
    feature_count=development_issues,
    coverage=(len(tests)/development_issues*100) if development_issues > 0 else 0,
    total_bugs=bugs_total,
    completed_features=df[(df['Issue Type'].isin(['Story', 'Task'])) & (df['Status'] == 'Done')].shape[0],
    bug_density=(bugs_total / max(1, df[(df['Issue Type'].isin(['Story', 'Task'])) & (df['Status'] == 'Done')].shape[0])),
    mttr_avg=avg_resolution_time if len(resolved_df) > 0 else 0,
    mttr_median=median_resolution_time if len(resolved_df) > 0 else 0,
    bugs_resolved=bugs_resolved,
    resolution_pct=bug_resolution_rate,
    test_exec_completed=test_exec_done if len(test_executions) > 0 else 0,
    test_exec_total=len(test_executions),
    test_completion_rate=test_exec_completion if len(test_executions) > 0 else 0,
    blocked_count=total_blocks,
    total_tickets=len(df),
    blocking_pct=(total_blocks/len(df)*100) if len(df) > 0 else 0
))

# ============================================================================
# RESUMEN EJECUTIVO Y RECOMENDACIONES
# ============================================================================
print("\n" + "=" * 100)
print("RESUMEN EJECUTIVO Y RECOMENDACIONES ESTRATÃ‰GICAS")
print("=" * 100)

print("""
ğŸ”´ PRIORIDADES CRÃTICAS (Actuar inmediatamente):

1. DEFECTOS EN PRODUCCIÃ“N
   â€¢ Bug Density actual: MONITOREAR
   â€¢ AcciÃ³n: Implementar "Bug Prevention" reviews antes de merge
   â€¢ Responsable: Jefe de Desarrollo
   â€¢ Timeline: PrÃ³ximas 2 semanas

2. COBERTURA DE TESTING
   â€¢ Target: Alcanzar 85%+ de cobertura
   â€¢ AcciÃ³n: Audit de features sin tests, crear plan de cobertura
   â€¢ Responsable: Lead de QA
   â€¢ Timeline: PrÃ³ximo sprint

3. BLOQUEOS DE DEPENDENCIAS
   â€¢ Current: Monitorear tickets bloqueados
   â€¢ AcciÃ³n: Daily standup enfocado en desbloqueos
   â€¢ Responsable: Scrum Master
   â€¢ Timeline: Semanal

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸŸ¡ OPORTUNIDADES DE MEJORA (prÃ³ximas 4 semanas):

1. VELOCIDAD DE CICLO
   â€¢ Implementar dashboards KPI en tiempo real
   â€¢ Monitorear tendencia de MTTR mes a mes
   â€¢ Establecer SLAs por tipo de issue

2. ESTIMACIONES Y PLANIFICACIÃ“N
   â€¢ Analizar accuracy ratio
   â€¢ Ajustar velocidad del team en base a datos histÃ³ricos
   â€¢ Implementar "estimation poker" con baseline histÃ³rico

3. CAPACIDAD DE QA
   â€¢ Evaluar si testing completion rate < 90% es por:
     - Falta de recursos
     - Procesos ineficientes
     - Casos de test complejos
   â€¢ Propuesta: AutomatizaciÃ³n de tests repetitivos

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸŸ¢ ACCIONES RECOMENDADAS (Plan de ImplementaciÃ³n):

SEMANA 1-2:
â–¡ Establecer baseline para los 5 KPIs
â–¡ Crear dashboard de monitoring en Jira/Grafana
â–¡ Comunicar targets y SLAs al equipo
â–¡ Establecer reuniones de revisiÃ³n semanal (viernes 2pm)

SEMANA 3-4:
â–¡ Implementar alertas automÃ¡ticas cuando KPIs salen de rango
â–¡ Crear reportes automatizados (diarios/semanales)
â–¡ SesiÃ³n de capacitaciÃ³n: "Definition of Done" actualizada
â–¡ Revisar procesos de code review y QA

SEMANA 5+:
â–¡ AnÃ¡lisis de tendencias de KPIs
â–¡ Identificar patrones (ej: ciertos features = mÃ¡s bugs)
â–¡ Ajuste de procesos basado en datos
â–¡ Escalada de riesgos identificados

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Exportar datos a CSV para anÃ¡lisis posterior
print("\nExportando datos para anÃ¡lisis posterior...")

# Resumen general
summary_data = {
    'MÃ©trica': [
        'Total de Tickets',
        'Tasa de Completitud (%)',
        'Tickets en Testing',
        'Total de Bugs',
        'Tasa ResoluciÃ³n Bugs (%)',
        'Test Cases',
        'Test Executions',
        'Tickets Bloqueados',
        'MTTR Promedio (dÃ­as)',
        'MTTR Mediana (dÃ­as)'
    ],
    'Valor': [
        len(df),
        f"{completion_rate:.1f}",
        f"{testing_issues}",
        f"{bugs_total}",
        f"{bug_resolution_rate:.1f}",
        len(tests),
        len(test_executions),
        f"{total_blocks}",
        f"{avg_resolution_time:.1f}" if len(resolved_df) > 0 else "N/A",
        f"{median_resolution_time:.1f}" if len(resolved_df) > 0 else "N/A"
    ]
}

summary_df = pd.DataFrame(summary_data)
summary_df.to_csv('KPI_Summary.csv', index=False)
print("âœ“ KPI_Summary.csv creado exitosamente")

# Exportar issues por estado
status_summary = df['Status'].value_counts().reset_index()
status_summary.columns = ['Estado', 'Cantidad']
status_summary.to_csv('Status_Distribution.csv', index=False)
print("âœ“ Status_Distribution.csv creado exitosamente")

# Exportar bugs abiertos
open_bugs = df[(df['Issue Type'] == 'Bug') & (df['Status'] != 'Done')][
    ['Issue key', 'Summary', 'Status', 'Priority', 'Assignee', 'Created']
].copy()
open_bugs.to_csv('Open_Bugs_Report.csv', index=False)
print("âœ“ Open_Bugs_Report.csv creado exitosamente")

print("\n" + "=" * 100)
print("ANÃLISIS COMPLETADO")
print("=" * 100)
